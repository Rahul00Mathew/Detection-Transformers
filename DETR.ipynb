{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "QqCd_Q0mjslj",
        "outputId": "6269bb44-9697-4823-d81c-14af236a27ae"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'cpu'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 63
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from pandas.core.common import flatten\n",
        "import numpy as np\n",
        "from numpy.ma.core import sqrt\n",
        "import random\n",
        "import json\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "from torchvision import datasets, transforms, models\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
        "\n",
        "import time\n",
        "\n",
        "import cv2\n",
        "\n",
        "import glob\n",
        "from tqdm import tqdm\n",
        "\n",
        "from torch import nn\n",
        "from torch import Tensor\n",
        "from PIL import Image\n",
        "from torchvision.transforms import Compose, Resize, ToTensor\n",
        "from torchvision.models import resnet50, resnet101\n",
        "from torchsummary import summary\n",
        "from torchvision.models._utils import IntermediateLayerGetter\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "device"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "frTDOuUNwvkX"
      },
      "source": [
        "### Sinusoidal Spatial Encoding\n",
        "https://github.com/tatp22/multidim-positional-encoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 163,
      "metadata": {
        "id": "-BNmVCuCdQtb"
      },
      "outputs": [],
      "source": [
        "def get_emb(sin_inp):\n",
        "  \"\"\"\n",
        "  Gets a base embedding for one dimension with sin and cos intertwined\n",
        "  \"\"\"\n",
        "  emb = torch.stack((sin_inp.sin(), sin_inp.cos()), dim=-1)\n",
        "  return torch.flatten(emb, -2, -1)\n",
        "\n",
        "\n",
        "class SinePositionalEncoding(nn.Module):\n",
        "  def __init__(self, channels):\n",
        "    super(SinePositionalEncoding, self).__init__()\n",
        "\n",
        "    self.org_channels = channels\n",
        "    channels = int(np.ceil(channels / 2) * 2)\n",
        "    self.channels = channels\n",
        "    inv_freq = 1.0 / (10000 ** (torch.arange(0, channels, 2).float() / channels))\n",
        "    self.register_buffer(\"inv_freq\", inv_freq)\n",
        "    self.register_buffer(\"cached_penc\", None)\n",
        "\n",
        "  def get_emb(self, sin_inp):\n",
        "    \"\"\"\n",
        "    Gets a base embedding for one dimension with sin and cos intertwined\n",
        "    \"\"\"\n",
        "    emb = torch.stack((sin_inp.sin(), sin_inp.cos()), dim=-1)\n",
        "    return torch.flatten(emb, -2, -1)\n",
        "\n",
        "  def forward(self, tensor):\n",
        "    if len(tensor.shape) != 3:\n",
        "      raise RuntimeError(\"The input tensor has to be 3d!\")\n",
        "\n",
        "    if self.cached_penc is not None and self.cached_penc.shape == tensor.shape:\n",
        "      return self.cached_penc\n",
        "\n",
        "    self.cached_penc = None\n",
        "    batch_size, seq_len, _ = tensor.shape\n",
        "    pos_x = torch.arange(seq_len, device=tensor.device).type(self.inv_freq.type())\n",
        "    sin_inp_x = torch.einsum(\"i, j->ij\", pos_x, self.inv_freq)\n",
        "\n",
        "    emb = self.get_emb(sin_inp_x)\n",
        "    self.cached_penc = emb.unsqueeze(0)\n",
        "\n",
        "    return self.cached_penc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RQDIWEpBkLsC"
      },
      "source": [
        "### CNN Backbone\n",
        "https://github.com/DanieleVeri/fair-DETR/blob/main/DETR.ipynb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 135,
      "metadata": {
        "id": "RTJxjHfpfJZZ"
      },
      "outputs": [],
      "source": [
        "class FrozenBatchNorm2d(torch.nn.Module):\n",
        "  \"\"\"\n",
        "  BatchNorm2d where the batch statistics and the affine parameters are fixed.\n",
        "\n",
        "  Copy-paste from torchvision.misc.ops with added eps before rqsrt,\n",
        "  without which any other models than torchvision.models.resnet[18,34,50,101]\n",
        "  produce nans.\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, n):\n",
        "    super(FrozenBatchNorm2d, self).__init__()\n",
        "    self.register_buffer(\"weight\", torch.ones(n))\n",
        "    self.register_buffer(\"bias\", torch.zeros(n))\n",
        "    self.register_buffer(\"running_mean\", torch.zeros(n))\n",
        "    self.register_buffer(\"running_var\", torch.ones(n))\n",
        "\n",
        "  def _load_from_state_dict(self, state_dict, prefix, local_metadata, strict,\n",
        "                              missing_keys, unexpected_keys, error_msgs):\n",
        "    num_batches_tracked_key = prefix + 'num_batches_tracked'\n",
        "    if num_batches_tracked_key in state_dict:\n",
        "      del state_dict[num_batches_tracked_key]\n",
        "\n",
        "    super(FrozenBatchNorm2d, self)._load_from_state_dict(\n",
        "        state_dict, prefix, local_metadata, strict,\n",
        "        missing_keys, unexpected_keys, error_msgs)\n",
        "\n",
        "  def forward(self, x):\n",
        "    # move reshapes to the beginning\n",
        "    # to make it fuser-friendly\n",
        "    w = self.weight.reshape(1, -1, 1, 1)\n",
        "    b = self.bias.reshape(1, -1, 1, 1)\n",
        "    rv = self.running_var.reshape(1, -1, 1, 1)\n",
        "    rm = self.running_mean.reshape(1, -1, 1, 1)\n",
        "    eps = 1e-5\n",
        "    scale = w * (rv + eps).rsqrt()\n",
        "    bias = b - rm * scale\n",
        "\n",
        "    return x * scale + bias\n",
        "\n",
        "\n",
        "class BackboneBase(nn.Module):\n",
        "  def __init__(self, backbone: nn.Module, train_backbone: bool, return_interm_layers: bool):\n",
        "    super(BackboneBase, self).__init__()\n",
        "\n",
        "    for name, parameter in backbone.named_parameters():\n",
        "      if not train_backbone or 'layer2' not in name and 'layer3' not in name and 'layer4' not in name:\n",
        "        parameter.requires_grad_(False)\n",
        "\n",
        "    if return_interm_layers:\n",
        "      return_layers = {'layer1': '0', 'layer2': '1', 'layer3': '2', 'layer4': '3'}\n",
        "    else:\n",
        "      return_layers = {'layer4': '0'}\n",
        "\n",
        "    self.body = IntermediateLayerGetter(backbone, return_layers=return_layers)\n",
        "\n",
        "  def forward(self, tensor):\n",
        "    out = self.body(tensor)\n",
        "    return out\n",
        "\n",
        "\n",
        "class Backbone(BackboneBase):\n",
        "  def __init__(self, name: str, train_backbone: bool, return_interm_layers: bool, dilation:bool):\n",
        "\n",
        "    backbone = getattr(models, name)(\n",
        "        replace_stride_with_dilation = [False, False, dilation],\n",
        "        pretrained = True,\n",
        "        norm_layer = FrozenBatchNorm2d\n",
        "    )\n",
        "\n",
        "    self.num_channels = 512 if name in ('resnet18', 'resnet34') else 2048\n",
        "    self.return_interm_layers = return_interm_layers\n",
        "    super().__init__(backbone, train_backbone, return_interm_layers)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Multi Head Attention Module\n",
        "https://www.datacamp.com/tutorial/building-a-transformer-with-py-torch"
      ],
      "metadata": {
        "id": "ieSnC7_ObZ5w"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "sny6XAnDV_n6"
      },
      "outputs": [],
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "  def __init__(self, d_model, num_heads):\n",
        "    super(MultiHeadAttention, self).__init__()\n",
        "\n",
        "    # Ensure d_model and num_heads are perfectly divisible\n",
        "    assert d_model % num_heads == 0, 'd_model must be divisible by num_heads'\n",
        "\n",
        "    # Initialize dimensions\n",
        "    self.d_model = d_model\n",
        "    self.num_heads = num_heads\n",
        "    self.d_k = d_model // num_heads\n",
        "\n",
        "    # Linear Layers to transform the inputs\n",
        "    self.W_q = nn.Linear(d_model, d_model)  # Key transformation\n",
        "    self.W_k = nn.Linear(d_model, d_model)  # Query transformation\n",
        "    self.W_v = nn.Linear(d_model, d_model)  # Value transformation\n",
        "    self.W_o = nn.Linear(d_model, d_model)  # Output transformation\n",
        "\n",
        "  def scaled_dot_product_attention(self, Q, K, V, mask=None):\n",
        "    # Calculate attention scores\n",
        "    attn_scores = torch.matmul(Q, K.transpose(-2, -1)) / np.sqrt(self.d_k)\n",
        "\n",
        "    # Apply mask if provided\n",
        "    if mask is not None:\n",
        "      attn_scores = attn_scores.masked_fill(mask == 0, 1e-9)\n",
        "\n",
        "    # Softmax is applied to attention scores to obtain attention probabilities\n",
        "    attn_probs = attn_scores.softmax(dim=-1)\n",
        "\n",
        "    # Multiply by value\n",
        "    output = torch.matmul(attn_probs, V)\n",
        "    return output\n",
        "\n",
        "  def split_heads(self, x):\n",
        "    # Split the input into num_heads for multi-head attention\n",
        "    batch_size, seq_len, d_model = x.shape\n",
        "    return x.view(batch_size, self.num_heads, seq_len, self.d_k)\n",
        "\n",
        "  def combine_heads(self, x):\n",
        "    # Combine the multiple heads back to original shape\n",
        "    batch_size, _, seq_len, d_k = x.shape\n",
        "    return x.view(batch_size, seq_len, self.d_model)\n",
        "\n",
        "  def forward(self, Q, K, V, mask=None):\n",
        "    # Apply Linear Transformations and split heads\n",
        "    Q = self.split_heads(self.W_q(Q))\n",
        "    K = self.split_heads(self.W_k(K))\n",
        "    V = self.split_heads(self.W_v(V))\n",
        "\n",
        "    # Scaled dot product attention\n",
        "    attn = self.scaled_dot_product_attention(Q, K, V, mask=None)\n",
        "    output = self.W_o(self.combine_heads(attn))\n",
        "\n",
        "    return output\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Feed Forward Network (with 2 layers)"
      ],
      "metadata": {
        "id": "Aai3Rg_QbmIi"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "_0JzD2D6tsbU"
      },
      "outputs": [],
      "source": [
        "class FeedForwardNetwrok(nn.Module):\n",
        "  def __init__(self, d_model, d_ffn):\n",
        "    super(FeedForwardNetwrok, self).__init__()\n",
        "\n",
        "    self.fc1 = nn.Linear(d_model, d_ffn)\n",
        "    self.fc2 = nn.Linear(d_ffn, d_model)\n",
        "    self.relu = nn.ReLU()\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.fc2(self.relu(self.fc1(x)))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Encoder"
      ],
      "metadata": {
        "id": "kt9txmllbtoD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "0jfWL9lepq2V"
      },
      "outputs": [],
      "source": [
        "class EncoderLayer(nn.Module):\n",
        "  def __init__(self, d_model, num_heads, d_ffn, dropout=0):\n",
        "    super(EncoderLayer, self).__init__()\n",
        "\n",
        "    self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
        "    self.ffn = FeedForwardNetwrok(d_model, d_ffn)\n",
        "    self.norm1 = nn.LayerNorm(d_model)\n",
        "    self.norm2 = nn.LayerNorm(d_model)\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "  def with_pos_enc(self, tensor, pos: None):\n",
        "    return tensor if pos is None else tensor + pos\n",
        "\n",
        "  def forward(self, src, pos, mask=None):\n",
        "    V = src\n",
        "    x = self.with_pos_enc(src, pos)\n",
        "    Q = K  = x\n",
        "\n",
        "    attn_output = self.self_attn(Q, K, V, mask)\n",
        "    out = self.norm1(x + self.dropout(attn_output))\n",
        "    ffn_output = self.ffn(out)\n",
        "    out = self.norm2(out + self.dropout(ffn_output))\n",
        "\n",
        "    return out"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Decoder"
      ],
      "metadata": {
        "id": "kizUpaFfbwTS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "id": "SSvM7MPiPJ6r"
      },
      "outputs": [],
      "source": [
        "class DecoderLayer(nn.Module):\n",
        "  def __init__(self, d_model, num_heads, d_ffn, dropout=0):\n",
        "    super(DecoderLayer, self).__init__()\n",
        "\n",
        "    self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
        "    self.cross_attn = MultiHeadAttention(d_model, num_heads)\n",
        "    self.ffn = FeedForwardNetwrok(d_model, d_ffn)\n",
        "    self.norm1 = nn.LayerNorm(d_model)\n",
        "    self.norm2 = nn.LayerNorm(d_model)\n",
        "    self.norm3 = nn.LayerNorm(d_model)\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "  def with_pos_enc(self, tensor, pos: None):\n",
        "    return tensor if pos is None else tensor + pos\n",
        "\n",
        "  def forward(self, tgt, enc_output, pos, query_pos, src_mask=None, tgt_mask=None):\n",
        "    Q = K = self.with_pos_enc(tgt, query_pos)\n",
        "    V = tgt\n",
        "    attn_output = self.self_attn(Q, K, V)\n",
        "    norm1_output = self.norm1(tgt + self.dropout(attn_output))\n",
        "\n",
        "    Q = self.with_pos_enc(norm1_output, query_pos)\n",
        "    K = self.with_pos_enc(enc_output, pos)\n",
        "    V = enc_output\n",
        "    attn_output = self.cross_attn(Q, K, V)\n",
        "    norm2_output = self.norm2(norm1_output + self.dropout(attn_output))\n",
        "\n",
        "    ffn_output = self.ffn(norm2_output)\n",
        "    out = self.norm3(norm2_output + self.dropout(ffn_output))\n",
        "\n",
        "    return out"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Transformer"
      ],
      "metadata": {
        "id": "nA-i3_g-byxR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "metadata": {
        "id": "C16wyQLp_rnz"
      },
      "outputs": [],
      "source": [
        "class Transformer(nn.Module):\n",
        "  def __init__(self, d_model, num_heads, num_encoder_layers, num_decoder_layers, d_ffn, return_intermediate=False, dropout=0):\n",
        "    super().__init__()\n",
        "\n",
        "    self.encoder_layers = nn.ModuleList([EncoderLayer(d_model, num_heads, d_ffn, dropout) for i in range(num_encoder_layers)])\n",
        "    self.decoder_layers = nn.ModuleList([DecoderLayer(d_model, num_heads, d_ffn, dropout) for i in range(num_decoder_layers)])\n",
        "\n",
        "    self._reset_parameters()\n",
        "\n",
        "    self.d_model = d_model\n",
        "    self.num_heads = num_heads\n",
        "    self.return_intermediate = return_intermediate\n",
        "\n",
        "  def _reset_parameters(self):\n",
        "    for p in self.parameters():\n",
        "      if p.dim() > 1:\n",
        "        nn.init.xavier_uniform_(p)\n",
        "\n",
        "  def forward(self, src, pos, query_pos):\n",
        "\n",
        "    enc_output = src\n",
        "    for encoder_layer in self.encoder_layers:\n",
        "      enc_output = encoder_layer(enc_output, pos)\n",
        "\n",
        "    tgt = torch.zeros_like(query_pos)\n",
        "    intermediate = []\n",
        "    dec_output = tgt\n",
        "    for decoder_layer in self.decoder_layers:\n",
        "      dec_output = decoder_layer(dec_output, enc_output, pos, query_pos)\n",
        "      if self.return_intermediate:\n",
        "        intermediate.append(dec_output)\n",
        "\n",
        "    if self.return_intermediate:\n",
        "      output = torch.stack(intermediate)\n",
        "    else:\n",
        "      output = dec_output.unsqueeze(0)\n",
        "\n",
        "    return output"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Multi Layer Perceptron Layer"
      ],
      "metadata": {
        "id": "26mAz5w1b_Rx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MLP(nn.Module):\n",
        "  \"\"\" Very simple multi-layer perceptron \"\"\"\n",
        "\n",
        "  def __init__(self, input_dim, hidden_dim, output_dim, num_layers):\n",
        "    super().__init__()\n",
        "    self.num_layers = num_layers\n",
        "    h = [hidden_dim] * (num_layers - 1)\n",
        "    self.layers = nn.ModuleList(nn.Linear(n, k) for n, k in zip([input_dim] + h, h + [output_dim]))\n",
        "\n",
        "  def forward(self, x):\n",
        "    for i, layer in enumerate(self.layers):\n",
        "      x = F.relu(layer(x)) if i < self.num_layers - 1 else layer(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "TRxnc_QQUS5s"
      },
      "execution_count": 122,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### DETR\n",
        "returns the classes and bounding boxes"
      ],
      "metadata": {
        "id": "Zzv0dlyscJ94"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DETR(nn.Module):\n",
        "  def __init__(self, backbone, transformer, num_classes, num_queries, aux_loss=False):\n",
        "    super().__init__()\n",
        "\n",
        "    self.num_queries = num_queries\n",
        "    self.transformer = transformer\n",
        "    self.hidden_dim = transformer.d_model\n",
        "    self.class_embed = nn.Linear(self.hidden_dim, num_classes + 1)\n",
        "    self.bbox_embed = MLP(self.hidden_dim, self.hidden_dim, 4, 3)\n",
        "    self.query_embed = nn.Embedding(num_queries, self.hidden_dim)\n",
        "    self.input_proj = nn.Conv2d(backbone.num_channels, self.hidden_dim, kernel_size=1)\n",
        "    self.backbone = backbone\n",
        "    self.position = SinePositionalEncoding(self.hidden_dim)\n",
        "    self.aux_loss = aux_loss\n",
        "\n",
        "  def forward(self, x):\n",
        "    if self.backbone.return_interm_layers:\n",
        "      features = self.backbone(x)['3']\n",
        "    else:\n",
        "      features = self.backbone(x)['0']\n",
        "\n",
        "    x = self.input_proj(features)\n",
        "    bs, _, _, _ = x.shape\n",
        "\n",
        "    src = x.view(bs, -1, self.hidden_dim)\n",
        "    pos = self.position(src)\n",
        "    query_pos = self.query_embed.weight.unsqueeze(0)\n",
        "\n",
        "    hs = self.transformer(src, pos, query_pos)\n",
        "\n",
        "    outputs_class = self.class_embed(hs)\n",
        "    outputs_coord = self.bbox_embed(hs).sigmoid()\n",
        "    out = {'pred_logits': outputs_class[-1], 'pred_boxes': outputs_coord[-1]}\n",
        "    if self.aux_loss:\n",
        "      out['aux_outputs'] = self._set_aux_loss(outputs_class, outputs_coord)\n",
        "    return out\n",
        "\n",
        "  def _set_aux_loss(self, outputs_class, outputs_coord):\n",
        "    # this is a workaround to make torchscript happy, as torchscript\n",
        "    # doesn't support dictionary with non-homogeneous values, such\n",
        "    # as a dict having both a Tensor and a list.\n",
        "    return [{'pred_logits': a, 'pred_boxes': b}\n",
        "            for a, b in zip(outputs_class[:-1], outputs_coord[:-1])]\n"
      ],
      "metadata": {
        "id": "hXU1UxbHR30q"
      },
      "execution_count": 162,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "backbone = Backbone('resnet50', False, False, False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tKzeoDikSQ61",
        "outputId": "0723ca5d-a0df-4349-9934-11ebfbfe8015"
      },
      "execution_count": 136,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "d_model = 512\n",
        "num_heads = 8\n",
        "num_encoder_layers = 6\n",
        "num_decoder_layers = 6\n",
        "d_ffn = 2048\n",
        "return_intermediate=False"
      ],
      "metadata": {
        "id": "Ybz-VPHTcn8e"
      },
      "execution_count": 156,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transformer = Transformer(d_model, num_heads, num_encoder_layers, num_decoder_layers, d_ffn, return_intermediate)"
      ],
      "metadata": {
        "id": "2plG04dGZ2Xi"
      },
      "execution_count": 157,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_classes = 91\n",
        "num_queries = 100"
      ],
      "metadata": {
        "id": "EYYQMKbmc5sW"
      },
      "execution_count": 158,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "detr = DETR(backbone, transformer, num_classes, num_queries)"
      ],
      "metadata": {
        "id": "mO9IfYvedAPa"
      },
      "execution_count": 164,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_test = torch.rand(2, 3, 800, 1000)"
      ],
      "metadata": {
        "id": "ncENIokodIG5"
      },
      "execution_count": 165,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y = detr(x_test)"
      ],
      "metadata": {
        "id": "GyAmKxgCdKPU"
      },
      "execution_count": 166,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y['pred_logits'].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RcZDOGU_dNpG",
        "outputId": "3955b500-c901-4571-8380-0c74f2d6e041"
      },
      "execution_count": 167,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([2, 100, 92])"
            ]
          },
          "metadata": {},
          "execution_count": 167
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y['pred_boxes'].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nmvSKzM3djNT",
        "outputId": "6beb738d-db31-45af-eaf0-260fa65b3f17"
      },
      "execution_count": 168,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([2, 100, 4])"
            ]
          },
          "metadata": {},
          "execution_count": 168
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IqNhRoqtdoax"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}